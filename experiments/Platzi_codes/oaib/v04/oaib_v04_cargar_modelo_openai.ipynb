{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWRLbJinfIm-",
        "outputId": "3af4af0f-e72a-4586-c6ae-834a16896cae"
      },
      "outputs": [],
      "source": [
        "# !pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JMiHUAbSgmsS"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîê Inicializaci√≥n segura del cliente OpenAI con variable de entorno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬°Cliente de OpenAI inicializado correctamente!\n"
          ]
        }
      ],
      "source": [
        "# AI_Queries/code_explanation/oaib/v01-script01.md\n",
        "#   AI_Queries/code_explanation/oaib/v01-script01_Tipos_de_modelos_de_OpenAI.md\n",
        "#   AI_Queries/code_explanation/oaib/v01-script01_diferencias‚ÄúModelos_de_Chat‚Äùy‚ÄúModelos_de_Completions‚Äù.md\n",
        "\n",
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "# Recuperar la clave API de la variable de entorno\n",
        "api_key_environ = os.environ.get(\"OPENAI_API_KEY\")\n",
        " \n",
        "# Verificar que la clave API est√© disponible\n",
        "if not api_key_environ:\n",
        "    raise ValueError(\"La variable de entorno OPENAI_API_KEY no est√° configurada o est√° vac√≠a.\")\n",
        " \n",
        "# Inicializar el cliente de OpenAI con la clave API\n",
        "client_openai = OpenAI(api_key=api_key_environ)\n",
        " \n",
        "# Usar el cliente para tus tareas\n",
        "print(\"¬°Cliente de OpenAI inicializado correctamente!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Ejemplo b√°sico de Chat Completion\n",
        "\n",
        "Primera llamada al endpoint `chat.completions.create`, sin par√°metros extra."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Am√©rica fue descubierta por Crist√≥bal Col√≥n, un navegante genov√©s al servicio de los Reyes Cat√≥licos de Espa√±a, el 12 de octubre de 1492.\n"
          ]
        }
      ],
      "source": [
        "# AI_Queries/knowledge/prompt_AI_GPT-Chat_completions_API_parameter.md\n",
        "# AI_Queries/knowledge/prompt_AI_GPT-Chat_Completions_API.md\n",
        "# AI_Queries/knowledge/prompt_AI_GTP-CodeExplanation01-chat_completions_API.md\n",
        "# AI_Queries/code_explanation/oaib/v01-script01_diferencias‚ÄúModelos_de_Chat‚Äùy‚ÄúModelos_de_Completions‚Äù.md\n",
        "# AI_Queries/code_explanation/oaib/v01-script01_estructura_interna_endpoint_de_Chat_Completions.md\n",
        "# AI_Queries/code_explanation/oaib/v01-script01_rangos-temperature,top_p,frequency_penalty,presence_penalty.md\n",
        "# AI_Queries/code_explanation/oaib/v01-script01_temperature,top_p,frequency_penalty,presence_penalty.md\n",
        "# AI_Queries/code_explanation/oaib/v01-script01_Tipos_de_modelos_de_OpenAI.md\n",
        "\n",
        "response = client_openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion a dudas\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¬øQui√©n descubri√≥ Am√©rica?\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat Completion con l√≠mite de tokens (max_tokens)\n",
        "\n",
        "Se a√±ade `max_tokens` para restringir la longitud de la respuesta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNKHA3Phgws3",
        "outputId": "7201bdec-ae38-43af-891d-ee86ed45384c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Am√©rica fue descubierta por Crist√≥bal Col√≥n en 1492, en su primer viaje financiado por los reyes de Espa√±a. Sin embargo, es importante recordar que el continente ya estaba habitado por diversas cult\n"
          ]
        }
      ],
      "source": [
        "response = client_openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion a dudas\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¬øQui√©n descubri√≥ Am√©rica?\"}\n",
        "    ],\n",
        "    max_tokens = 50,\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Respuesta truncada**  \n",
        "El texto finaliza abruptamente en ‚Äúcult‚Ä¶‚Äù, lo que indica que se alcanz√≥ el l√≠mite de tokens permitido (o se cort√≥ por defecto)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat Completion con muestreo y respuestas m√∫ltiples\n",
        "\n",
        "Se incorporan `temperature`, `top_p` y `n=2` para controlar la aleatoriedad y obtener dos salidas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9253KN3KNJKf",
        "outputId": "e2264882-20ea-4d55-bd6a-0b2ebf2bfcb2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "choices[0] es \t:Am√©rica fue descubierta por Crist√≥bal Col√≥n en 1492 durante su primer viaje financiado por los Reyes Cat√≥licos de Espa√±a.\n",
            "\n",
            "choices[1] es \t:Am√©rica fue descubierta por Crist√≥bal Col√≥n en 1492, en su expedici√≥n financiada por los Reyes Cat√≥licos de Espa√±a.\n",
            "\n",
            "ChatCompletion(id='chatcmpl-BfsyWW0FpPdGEzv6P9sT7aBWewidG', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Am√©rica fue descubierta por Crist√≥bal Col√≥n en 1492 durante su primer viaje financiado por los Reyes Cat√≥licos de Espa√±a.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[])), Choice(finish_reason='stop', index=1, logprobs=None, message=ChatCompletionMessage(content='Am√©rica fue descubierta por Crist√≥bal Col√≥n en 1492, en su expedici√≥n financiada por los Reyes Cat√≥licos de Espa√±a.', refusal=None, role='assistant', audio=None, function_call=None, tool_calls=None, annotations=[]))], created=1749322472, model='gpt-3.5-turbo-0125', object='chat.completion', service_tier='default', system_fingerprint=None, usage=CompletionUsage(completion_tokens=67, prompt_tokens=32, total_tokens=99, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
          ]
        }
      ],
      "source": [
        "response = client_openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion a dudas\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¬øQui√©n descubri√≥ Am√©rica?\"}\n",
        "    ],\n",
        "    max_tokens = 50,\n",
        "    temperature = 1,\n",
        "    top_p = 1,\n",
        "    n = 2\n",
        ")\n",
        "# n=2 produce la generaci√≥n de dos respuestas\n",
        "print(f\"choices[0] es \\t:{response.choices[0].message.content}\\n\")\n",
        "print(f\"choices[1] es \\t:{response.choices[1].message.content}\\n\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Estas son algunas anotaciones espec√≠ficas tras revisar la salida de:\n",
        "\n",
        "* **M√≠nima diversidad entre elecciones**\n",
        "  A pesar de haber usado `n=2` y `temperature=1`, ambas respuestas son pr√°cticamente id√©nticas.\n",
        "\n",
        "  * Implicaci√≥n: esta pregunta factual es ‚Äúde baja entrop√≠a‚Äù, por lo que el modelo tiende a repetir la misma formulaci√≥n.\n",
        "  * Acci√≥n sugerida: si necesitas variaciones m√°s notables, podr√≠as usar prompts m√°s abiertos, subir `temperature` a√∫n m√°s (por ejemplo 1.2 en modelos que lo permitan) o ajustar `top_p` (por ejemplo a 0.8).\n",
        "\n",
        "* **Terminaci√≥n normal (`finish_reason='stop'`)**\n",
        "  El hecho de que ambas elecciones terminen con `finish_reason='stop'` indica que la generaci√≥n se complet√≥ al alcanzar una secuencia de parada o al concluir naturalmente, no por l√≠mite de tokens.\n",
        "\n",
        "* **Tokens usados y coste**\n",
        "\n",
        "  ```\n",
        "  prompt_tokens=32  \n",
        "  completion_tokens=67  \n",
        "  total_tokens=99  \n",
        "  ```\n",
        "\n",
        "  Registra estos valores para estimar el coste y ajustar `max_tokens` en futuras pruebas (por ejemplo si quisieras respuestas m√°s largas o m√°s cortas).\n",
        "\n",
        "* **Modelo en uso**\n",
        "  Se informa `model='gpt-3.5-turbo-0125'`, lo cual confirma que, aunque especificaste `\"gpt-3.5-turbo\"`, el sistema ha concretado la versi√≥n `-0125`. Sirve para dejar constancia de la versi√≥n exacta de modelo probada.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Respuesta 1: Am√©rica fue descubierta por Crist√≥bal Col√≥n en 1492 durante su primer viaje financiado por los Reyes Cat√≥licos de Espa√±a.\n",
            "Respuesta 2: Am√©rica fue descubierta por Crist√≥bal Col√≥n en 1492, en su expedici√≥n financiada por los Reyes Cat√≥licos de Espa√±a.\n"
          ]
        }
      ],
      "source": [
        "# mejor forma de presentar el resultado anterior\n",
        "for i, choice in enumerate(response.choices):\n",
        "    print(f\"Respuesta {i+1}: {choice.message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chat Completion con temperatura baja y respuestas m√∫ltiples\n",
        "\n",
        "Igual que el anterior, pero con `temperature=0.3` para respuestas m√°s deterministas (menos ‚Äúcreativas‚Äù)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IA son las siglas de Inteligencia Artificial, que se refiere a la simulaci√≥n de procesos de inteligencia humana por parte de m√°quinas, especialmente sistemas inform√°ticos. La IA se utiliza en una amplia variedad de aplicaciones, como reconocimiento de voz, an√°lisis de datos, conducci√≥n aut√≥noma, entre otros.\n"
          ]
        }
      ],
      "source": [
        "response = client_openai.chat.completions.create(\n",
        "    model = 'gpt-3.5-turbo',\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\" :\"Eres un asistente que da informacion a dudas\"},\n",
        "        {\"role\": \"user\", \"content\" :\"¬øQu√© es IA?\"}\n",
        "    ],\n",
        "    max_tokens = 100,\n",
        "    temperature = 0.3,\n",
        "    top_p = 1,\n",
        "    n = 1\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Algunas anotaciones tras revisar la salida:\n",
        "\n",
        "* **Alta coherencia y determinismo**\n",
        "  Con `temperature=0.3` el modelo genera respuestas muy parecidas y estables. En preguntas definidas como ‚Äú¬øQu√© es IA?‚Äù, resulta acertado porque da un enunciado claro y conciso.\n",
        "\n",
        "* **Respuesta completa y sin truncar**\n",
        "  El contenido est√° bien formado y no se ha cortado en medio de una frase; por tanto, `max_tokens=100` fue suficiente en este caso.\n",
        "\n",
        ">* **Validar `finish_reason`**\n",
        "  Comprueba que `response.choices[0].finish_reason == \"stop\"` para confirmar que termin√≥ de forma natural y no por l√≠mite de tokens.\n",
        "\n",
        ">* **Registro de uso de tokens**\n",
        "  Extrae `response.usage` para anotar cu√°ntos tokens consumi√≥ el prompt y la respuesta. Esto te ayudar√° a optimizar costes en futuras pruebas.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.choices[0].finish_reason == \"stop\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "CompletionUsage(completion_tokens=148, prompt_tokens=29, total_tokens=177, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.usage"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "UNAD-dBjyLoWd",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
