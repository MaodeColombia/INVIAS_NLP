{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "\n",
    "def extract_xmp_metadata(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts the XMP metadata manually from a PDF file by parsing its contents.\n",
    "\n",
    "    Args:\n",
    "        pdf_path (str): Path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        str: The XMP metadata as a string (XML format), or None if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Open the PDF file\n",
    "        document = fitz.open(pdf_path)\n",
    "        \n",
    "        # Iterate over all objects in the PDF to find the XMP metadata\n",
    "        for xref in range(1, document.xref_length()):\n",
    "            stream = document.xref_stream(xref)\n",
    "            if stream and b'<x:xmpmeta' in stream:\n",
    "                start = stream.index(b'<x:xmpmeta')\n",
    "                end = stream.index(b'</x:xmpmeta>') + len(b'</x:xmpmeta>')\n",
    "                xmp_metadata = stream[start:end].decode(\"utf-8\")\n",
    "                document.close()\n",
    "                return xmp_metadata\n",
    "        \n",
    "        document.close()\n",
    "        return None  # No XMP metadata found\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting XMP metadata: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    pdf_file_path = \"../assets/2024S-VBOG-053309.pdf\"  # Replace with the path to your PDF file\n",
    "    xmp_metadata = extract_xmp_metadata(pdf_file_path)\n",
    "    \n",
    "    if xmp_metadata:\n",
    "        print(\"XMP Metadata:\")\n",
    "        print(xmp_metadata)\n",
    "    else:\n",
    "        print(\"No XMP metadata found or error occurred.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Descargar las stop words de NLTK si no las tienes\n",
    "'''nltk.download('stopwords')'''\n",
    "\n",
    "# Cargar el modelo de spaCy\n",
    "nlp = spacy.load(\"es_core_news_sm\")\n",
    "\n",
    "# Obtener las stop words en español de NLTK\n",
    "stop_words = set(stopwords.words('spanish'))\n",
    "\n",
    "def procesar_texto(texto):\n",
    "    \"\"\"\n",
    "    Realiza pre-procesamiento, tokenización, eliminación de stop words y lematización.\n",
    "\n",
    "    Args:\n",
    "        texto: El texto extraído del PDF.\n",
    "\n",
    "    Returns:\n",
    "        Una lista de tokens lematizados, sin stop words ni signos de puntuación.\n",
    "    \"\"\"\n",
    "    doc = nlp(texto)\n",
    "    tokens_lematizados = []\n",
    "    for token in doc:\n",
    "        # Filtrar stop words y signos de puntuación\n",
    "        if token.text.lower() not in stop_words and not token.is_punct:\n",
    "            tokens_lematizados.append(token.lemma_)\n",
    "    return tokens_lematizados\n",
    "\n",
    "# Ejemplo de uso:\n",
    "texto_ejemplo = \"El análisis de los documentos PDF, como los reportes financieros, es esencial. Los analistas necesitan extraer información clave.\"\n",
    "tokens = procesar_texto(texto_ejemplo)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def etiquetado_gramatical(texto):\n",
    "    \"\"\"\n",
    "    Realiza el etiquetado gramatical del texto.\n",
    "\n",
    "    Args:\n",
    "        texto: El texto a procesar.\n",
    "\n",
    "    Returns:\n",
    "        Una lista de tuplas, donde cada tupla contiene el token y su etiqueta gramatical.\n",
    "    \"\"\"\n",
    "    doc = nlp(texto)\n",
    "    etiquetas = [(token.text, token.pos_) for token in doc]\n",
    "    return etiquetas\n",
    "\n",
    "# Ejemplo de uso:\n",
    "texto_ejemplo = \"El gato negro corre rápidamente.\"\n",
    "etiquetas = etiquetado_gramatical(texto_ejemplo)\n",
    "print(etiquetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extraer_entidades(texto):\n",
    "    \"\"\"\n",
    "    Extrae las entidades nombradas del texto.\n",
    "\n",
    "    Args:\n",
    "        texto: El texto a procesar.\n",
    "\n",
    "    Returns:\n",
    "        Una lista de tuplas, donde cada tupla contiene la entidad y su tipo.\n",
    "    \"\"\"\n",
    "    doc = nlp(texto)\n",
    "    entidades = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entidades\n",
    "\n",
    "# Ejemplo de uso:\n",
    "texto_ejemplo = \"Apple Inc. reportó ingresos récord en su sede de Cupertino en el último trimestre de 2023.\"\n",
    "entidades = extraer_entidades(texto_ejemplo)\n",
    "print(entidades)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "def resumir_texto(texto, modelo=\"google/pegasus-xsum\"):\n",
    "    \"\"\"\n",
    "    Resume el texto usando el modelo Pegasus-XSum.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Carga el pipeline de resumen\n",
    "        resumidor = pipeline(\"summarization\", model=modelo, tokenizer=modelo)\n",
    "        \n",
    "        # Realiza el resumen\n",
    "        resultado = resumidor(texto, truncation=True, max_length=150, min_length=40)\n",
    "        return resultado\n",
    "    except Exception as e:\n",
    "        print(f\"Ocurrió un error al resumir el texto: {e}\")\n",
    "        return None\n",
    "\n",
    "# Ejemplo de uso\n",
    "texto = \"\"\"\n",
    "Apple Inc. es una empresa multinacional estadounidense de tecnología con sede en Cupertino, California.\n",
    "Diseña, fabrica y comercializa electrónica de consumo, software informático y servicios en línea.\n",
    "La compañía fue fundada por Steve Jobs, Steve Wozniak y Ronald Wayne en abril de 1976 para desarrollar y vender computadoras personales Apple I de Wozniak.\n",
    "Fue incorporada como Apple Computer, Inc. en enero de 1977, y las ventas de sus computadoras, incluyendo el Apple II, crecieron rápidamente.\n",
    "Apple salió a bolsa en 1980 con un éxito financiero instantáneo.\n",
    "\"\"\"\n",
    "resumen = resumir_texto(texto)\n",
    "\n",
    "# Imprimir el resumen\n",
    "if resumen:\n",
    "    print(\"Resumen:\")\n",
    "    print(resumen[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Especifica la carpeta que quieres listar\n",
    "carpeta =  r\"C:\\Users\\mmartinezo\\OneDrive - Instituto Nacional de Vias - INVIAS\\Mettings\\Reuniones\\Recordings\"\n",
    "# Lista los archivos en la carpeta\n",
    "archivos = os.listdir(carpeta)\n",
    "\n",
    "# Filtra solo los archivos (excluyendo subcarpetas)\n",
    "archivos = [archivo for archivo in archivos if os.path.isfile(os.path.join(carpeta, archivo))]\n",
    "\n",
    "# Imprime la lista de archivos\n",
    "x=0\n",
    "for archivo in archivos:\n",
    "    x = x+1\n",
    "    print(x)\n",
    "    print(archivo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNAD-dBjyLoWd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
