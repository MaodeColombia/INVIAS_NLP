{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evitar que se corra `!pip install openai` cada vez que se corre el c√≥digo; c√≥digo generado por ChatGPT. Explicaci√≥n (AI_Queries\\prompt_AI_GPT-openai_install_upgrade.md)\n",
    "\n",
    "import importlib\n",
    "import subprocess\n",
    "import pkg_resources\n",
    "from packaging import version\n",
    "\n",
    "def ensure_package(package_name):\n",
    "    try:\n",
    "        # Check if the package is installed\n",
    "        pkg = importlib.import_module(package_name)\n",
    "        installed_version = pkg_resources.get_distribution(package_name).version\n",
    "        print(f\"‚úÖ {package_name} is already installed (version: {installed_version}).\")\n",
    "\n",
    "        # Check if the package is up-to-date\n",
    "        result = subprocess.run(\n",
    "            [\"pip\", \"index\", \"versions\", package_name],\n",
    "            capture_output=True,\n",
    "            text=True\n",
    "        )\n",
    "\n",
    "        if result.returncode == 0 and \"Available versions:\" in result.stdout:\n",
    "            latest_version = result.stdout.split(\"Available versions:\")[-1].split(\"\\n\")[0].split(\",\")[0].strip()\n",
    "            if version.parse(installed_version) < version.parse(latest_version):\n",
    "                print(f\"‚ö†Ô∏è A new version of {package_name} is available: {latest_version}.\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {package_name} is up-to-date.\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è Unable to fetch the latest version. Please check your network.\")\n",
    "    except ModuleNotFoundError:\n",
    "        # Install the package if not found\n",
    "        print(f\"üîÑ Installing {package_name}...\")\n",
    "        subprocess.check_call([\"pip\", \"install\", package_name])\n",
    "        print(f\"‚úÖ {package_name} has been installed.\")\n",
    "\n",
    "# Check, install, or update the 'openai' package\n",
    "ensure_package(\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mantener actualizado el paquete `openai`\n",
    "\n",
    "import subprocess\n",
    "\n",
    "def update_package(package_name):\n",
    "    print(f\"üîÑ Updating {package_name} to the latest version...\")\n",
    "    subprocess.check_call([\"pip\", \"install\", \"--upgrade\", package_name])\n",
    "    print(f\"‚úÖ {package_name} has been updated to the latest version!\")\n",
    "\n",
    "# Update the 'openai' package\n",
    "# update_package(\"openai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Configuraci√≥n inicial\n",
    "from openai import OpenAI, AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Creaci√≥n de la variable de entorno para la 'OPENAI_API_KEY'. Explicaci√≥n \"./AI_Queries/prompt_AI_GPT-Environment_variable_creation.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recuperar la clave API de la variable de entorno\n",
    "api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Verificar que la clave API est√© disponible\n",
    "if not api_key:\n",
    "    raise ValueError(\"La variable de entorno OPENAI_API_KEY no est√° configurada o est√° vac√≠a.\")\n",
    "\n",
    "# Inicializar el cliente de OpenAI con la clave API\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "# Usar el cliente para tus tareas\n",
    "print(\"¬°Cliente de OpenAI inicializado correctamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"¬°Hola! ¬øMe ayudas a aprender sobre la API de OpenAI?\"\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Si este script produce error probalblemente sea que no hay credito para hacer la consultas a la API, ver https://platform.openai.com/settings/organization/billing/overview\n",
    "\n",
    "# ./assets/images/20250103_150601.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la respuesta del modelo `gpt-4o-mini`; explicaci√≥n approach_1\\outputs\\ChatCompletionMessage_explanation.md\n",
    "completion.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Presentaci√≥n del `content` de `completion.choices[0].message` en formato markdown dentro del notebook. \n",
    "from IPython.display import display,Markdown\n",
    "display(Markdown(completion.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Los Roles de *Chat_Completions_API* (client.chat.completions) tiene tres roles; explicacion AI_Queries\\prompt_AI_GPT-Chat_Completions_API.md\n",
    "# Los par√°metros adicionales que permiten personalizar la interacci√≥n con los modelos de chat *Chat_Completions_API* (client.chat.completions) se explican en AI_Queries\\prompt_AI_GPT-Chat_completions_API_parameter.md\n",
    "\n",
    "# El siguiente codigo se explica en AI_Queries\\prompr_AI_GTP-CodeExplanation01-chat_completions_API.md\n",
    "\n",
    "# 3. Agregar un system prompt\n",
    "def chat_with_system(system_prompt: str, user_prompt: str) -> str:\n",
    "    \"\"\"Realiza una llamada con system prompt\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de uso\n",
    "system_prompt = \"Eres un asistente con increible sentido del humor, que hace chistes de las tematicas que te solicitan, ademas tu acento es un muy marcado Argentino\"\n",
    "user_prompt = \"algo de borrachos\"\n",
    "respuesta = chat_with_system(system_prompt, user_prompt)\n",
    "print(f\"\\nSystem Prompt: {system_prompt}\")\n",
    "print(f\"User Prompt: {user_prompt}\")\n",
    "print(f\"Respuesta: {respuesta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "¬°Claro, parce! Aqu√≠ te va uno:\n",
       "\n",
       "Est√°n dos borrachos en la calle y uno le dice al otro:\n",
       "\n",
       "‚Äî ¬°Oye! ¬°Si tomamos m√°s, vamos a borrar todo lo que hicimos anoche!\n",
       "\n",
       "Y el otro le responde: \n",
       "\n",
       "‚Äî ¬°Uy, como eso se vea, me voy a empezar a escribir un diario! üòÇüçª\n",
       "\n",
       "¬°Salud!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creaci√≥n de lista para almacenar mensajes para manejar un hist√≥rico de la conversaci√≥n\n",
    "collated_messages =  []\n",
    "\n",
    "# recordando que la estructura para el uso de la API es\n",
    "# client.chat.completions.create(model=\"   \", messages=[{\"role\":\"system\",\"content\":\"   \"},{\"role\":\"user\",\"content\":\"   \"}])\n",
    "\n",
    "\n",
    "# Creaci√≥n del system prompt inicial y system user inicial\n",
    "collated_messages.append(\n",
    "    {\n",
    "        \"role\" : \"system\",\n",
    "        \"content\" : \"Eres un asistente con increible sentido del humor, que hace chistes de las tematicas que te solicitan, ademas tu acento es un muy marcado Colombiano\"\n",
    "    }\n",
    ")\n",
    "\n",
    "collated_messages.append(\n",
    "    {\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : \"Un chiste de borrachos\"\n",
    "    }\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=collated_messages)\n",
    "\n",
    "# De solo este bloque de c√≥digo, hasta esta linea, solo se ha obtenido la primera respuesta a una consulta del modelo gpt-4o-mini, la idea es almacenar la respuesta en collated_messages, teniendo presente que la respuesta dada por el modelo se almacena como \"assitant\"\n",
    "collated_messages.append(\n",
    "    {\n",
    "        \"role\" : \"assistant\",\n",
    "        \"content\" : response.choices[0].message.content\n",
    "    }\n",
    ")\n",
    "\n",
    "display(Markdown(collated_messages[2][\"content\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "UNAD-dBjyLoWd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
