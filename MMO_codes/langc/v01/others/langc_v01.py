# %% [markdown]
# # LangChain para INVIAS

# %% [markdown]
# ## üîí 1. Instalaci√≥n de librer√≠as

# %% [markdown]
# ### üîì  Solo ejecutar este script si no se han instalado los paquetes para desarrollar el c√≥digo

# %%
""" 
Lista de los paquetes a instalar:

    upgrade:
        pip
        setuptools
        wheel
    Packages:
        langchain 
        pypdf 
        openai 
        chromadb 
        tiktoken
        langchain-community
"""
import subprocess

comandos = [
    ["pip", "install", "--upgrade", "pip", "setuptools", "wheel"],
    ["pip", "install", "langchain", "pypdf", "openai", "chromadb", "tiktoken"],
    ["pip", "install", "-U", "langchain-community"],
    ["python", "-m", "pip", "install", "--upgrade", "pip"]
]

log_path = "instalacion_log.txt"

with open(log_path, "w", encoding="utf-8") as log_file:
    for i, cmd in enumerate(comandos, start=1):
        log_file.write(f"\nüîß Ejecutando comando {i}: {' '.join(cmd)}\n")
        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        
        log_file.write("‚úÖ STDOUT:\n")
        log_file.write(result.stdout + "\n")
        
        if result.stderr:
            log_file.write("‚ö†Ô∏è STDERR:\n")
            log_file.write(result.stderr + "\n")

print(f"‚úÖ Resultado guardado en {log_path}")


# %% [markdown]
# ## 2. Configuraci√≥n de API Key de OpenAI

# %%
import os
from openai import OpenAI
# Recuperar la clave API de la variable de entorno
api_key_environ = os.environ.get("OPENAI_API_KEY")
 
# Verificar que la clave API est√© disponible
if not api_key_environ:
    raise ValueError("La variable de entorno OPENAI_API_KEY no est√° configurada o est√° vac√≠a.")
 
# Inicializar el cliente de OpenAI con la clave API
client = OpenAI(api_key=api_key_environ)
 
# Usar el cliente para tus tareas
print("¬°Cliente de OpenAI inicializado correctamente!")

# %% [markdown]
# ## üîí 3. Carga de documents

# %% [markdown]
# ### üîì  Solo ejecutar este script si no se ha hecho el proceso de embedding

# %%
import requests
from langchain.document_loaders import PyPDFLoader
import os

relative_pdf_path = "../../../assets/DG_docs/PDF_test_gradio/"

ml_papers = []

for i, file_name in enumerate(os.listdir(relative_pdf_path)):
    if file_name.lower().endswith(".pdf"):
        full_pdf_path = os.path.join(relative_pdf_path,file_name)
        print(f"üìÑ Cargando {file_name}")

        loader = PyPDFLoader(full_pdf_path)
        data = loader.load() # AI_Queries\code_explanation\ai_query-langc_v01-PyPDFLoader(filename).loader.load()_usage.md
        ml_papers.extend(data) # AI_Queries\code_explanation\ai_query-langc_v01-.extend_usage.md
        # print (ml_papers) # AI_Queries/code_explanation/ai_query-langc_v01-list_start_end_usage.md
# Utiliza la lista ml_papers para acceder a los elementos de todos los documentos descargados
print('Esto es todo el contenido de `ml_papers:`')
print(f"""
üÜó Todos los documentos estan cargados en ml_papers.
‚ûñ Total de fragmentos: {len(ml_papers)}
‚ûñ Los fragmentos son cada una de las hojas de cada uno de los {len(os.listdir(relative_pdf_path))} archivos en la carpeta {relative_pdf_path}
‚ûñ Este script se ejecuta desde {os.getcwd()}
‚ûñ Este es el contenido de la √∫ltima hoja cargada {ml_papers[-1]}
""")

# %% [markdown]
# ## üîí 4. Split de documents

# %% [markdown]
# ### üîì
# 
# Solo ejecutar este script para desarrollar el proceso de embedding; este script depende del script anterior 

# %%
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1500, 
    # AI_Queries\code_explanation\ai_query-langc_v01-chunk_usage.md 
    # AI_Queries\code_explanation\ai_query-langc_v01-max_tokens_Chatgptmodels.md 
    # AI_Queries\code_explanation\ai_query-langc_v01-meaning_inputpromptandanswer.md 
    # AI_Queries\code_explanation\ai_query-langc_v01-retrieval_meaning.md
    
    chunk_overlap=200,
    length_function=len
    )

documents = text_splitter.split_documents(ml_papers)

# %%
len(documents), documents[0]

# %% [markdown]
# ## üîí 5. Embeddings e ingesta a base de datos vectorial 
# 
# ‚ö†Ô∏è advertencia de uso de esta secci√≥n ‚ö†Ô∏è

# %% [markdown]
# ### üîì 5.1.
# 
# Solo ejecutar este script para desarrolla el proceso de embedding; este script depende del script anterior
# 
# **Aqu√≠ se consume recurso de la API de OpenAI**

# %%
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# AI_Queries\code_explanation\ai_query-langc_v01-Embeddings_and_Vector_Store_Ingestion.md

# 1. Crear embeddings con el modelo oficial de OpenAI
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002") # ‚ö†Ô∏è cambiar a "text-embedding-3-small"

# 2. Definir carpeta para almacenar la base de datos vectorial
persist_directory = "chroma_db" #

# 3. Crear la base desde documentos y embeddings
vectorstore = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,
    persist_directory=persist_directory
)

# 4. Guardar la base en disco
vectorstore.persist()
print("‚úÖ Base de datos Chroma guardada en:", persist_directory)


# 5. Cargar la base vectorial guardada en disco
vectorstore = Chroma(
    embedding_function=embeddings,
    persist_directory="chroma_db"
)

# 6. Usar como retriever
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 3}
    )

# %% [markdown]
# ### 5.2.
# 
# Fue necesario crear copia de las lineas "*# 1. Crear embeddings con el modelo oficial de OpenAI*", "*# 5. Cargar la base vectorial guardada en disco*" y "*# 6. Usar como retriever*" para **solamente hacer uso de la Based de Datos de embeddings `chroma_db` ya creada y evitar recalcularla**.

# %%
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma

# 1. Modelo de embeddings (debe ser el mismo usado al crear la base)
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")  # ‚ö†Ô∏è cambiar a "text-embedding-3-small"

# 5. Cargar la base vectorial guardada en disco
vectorstore = Chroma(
    embedding_function=embeddings,
    persist_directory="chroma_db"
)

# 6. Usar como retriever
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 5}
)

# %% [markdown]
# ## 6. Modelos de chat y cadenas para consulta de informaci√≥n

# %% [markdown]
# ### üö© 6.1. Recuperaci√≥n sin trazabilidad ‚Äì Primer acercamiento con LangChain; ~~No cita las fuentes de donde extrae la informaci√≥n~~.

# %%
#AI_Queries/code_explanation/ai_query-langc_v01-Chat_Models_and_Retrieval_Chains_for_Information_Querying.md

from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

chat = ChatOpenAI(
    openai_api_key=api_key_environ,
    model_name='gpt-3.5-turbo',
    temperature=0.0
)

# AI_Queries/code_explanation/ai_query-langc_v01-RetrievalQA.from_chain_type_usage.md
qa_chain = RetrievalQA.from_chain_type(
    llm=chat,
    chain_type="stuff",
    retriever=retriever
)

# %%
# AI_Queries/code_explanation/ai_query-langc_v01-qa_chain.run()_usage.md

query = "qu√© es CCPT?"
qa_chain.run(query)

# %% [markdown]
# üß† Comparaci√≥n entre Script 1 y Script 2
# 
# | Aspecto                           | **Script anterior**                                 | **Script siguiente**                                                       |
# | --------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------- |
# | **Importaci√≥n del modelo**        | `from langchain.chat_models` (obsoleto)             | `from langchain_openai` (recomendado)                                      |
# | **Versi√≥n del API**               | Antiguo LangChain monol√≠tico                        | Nuevo ecosistema modular `langchain-openai`                                |
# | **M√©todo de ejecuci√≥n**           | `qa_chain.run(query)` (‚ö†Ô∏è obsoleto)                 | `qa_chain.invoke({"query": query})` (‚úÖ recomendado)                        |
# | **Fuentes del resultado**         | ‚ùå No devuelve los documentos fuente                 | ‚úÖ Incluye los documentos fuente con `return_source_documents=True`         |
# | **Retorno estructurado**          | S√≥lo devuelve texto plano                           | Devuelve un diccionario con respuesta y `source_documents`                 |
# | **Transparencia institucional**   | Baja (no se puede verificar origen de la respuesta) | Alta (permite validar en qu√© parte de los documentos se basa la respuesta) |
# | **Uso pedag√≥gico recomendado**    | Para introducir conceptos b√°sicos de `RetrievalQA`  | Para ense√±ar buenas pr√°cticas actuales en trazabilidad y uso del LLM       |
# | **Estabilidad futura del c√≥digo** | Baja (usa m√≥dulos y m√©todos deprecados)             | Alta (adaptado a la versi√≥n estable y mantenida de LangChain)              |

# %% [markdown]
# ### (üö©Revisar por qu√© no est√° gerendo resultadosüö©) 6.2. Con recuperaci√≥n b√°sica ‚Äì El sistema responde y cita fuentes

# %%
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA

# Inicializa el modelo de lenguaje
chat = ChatOpenAI(
    model="gpt-3.5-turbo",
    temperature=0.0,
    openai_api_key=api_key_environ
)

# Crear la cadena de preguntas y respuestas con fuentes
qa_chain = RetrievalQA.from_chain_type(
    llm=chat,
    chain_type="stuff",
    retriever=retriever,
    return_source_documents=True  # ‚úÖ Para obtener las fuentes
)

# %%
query = " que pasa en Oca√±a?"
result = qa_chain.invoke({"query": query})

# Imprimir respuesta
print("üß† Respuesta:")
print(result['result'])

# Imprimir fuentes
print("\nüìö Fuentes:")
for i, doc in enumerate(result['source_documents']):
    print(f"\nFuente {i+1}:")
    print("Archivo:", doc.metadata.get("source", "desconocido"))
    print("Contenido:", doc.page_content[:400])  # Muestra primeros 400 caracteres


# %% [markdown]
# üß† Comparaci√≥n entre Script 1 y Script 2
# 
# | Aspecto                           | **Script anterior**                                 | **Script actual**                                                       |
# | --------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------- |
# | **Importaci√≥n del modelo**        | `from langchain.chat_models` (obsoleto)             | `from langchain_openai` (recomendado)                                      |
# | **Versi√≥n del API**               | Antiguo LangChain monol√≠tico                        | Nuevo ecosistema modular `langchain-openai`                                |
# | **M√©todo de ejecuci√≥n**           | `qa_chain.run(query)` (‚ö†Ô∏è obsoleto)                 | `qa_chain.invoke({"query": query})` (‚úÖ recomendado)                        |
# | **Fuentes del resultado**         | ‚ùå No devuelve los documentos fuente                 | ‚úÖ Incluye los documentos fuente con `return_source_documents=True`         |
# | **Retorno estructurado**          | S√≥lo devuelve texto plano                           | Devuelve un diccionario con respuesta y `source_documents`                 |
# | **Transparencia institucional**   | Baja (no se puede verificar origen de la respuesta) | Alta (permite validar en qu√© parte de los documentos se basa la respuesta) |
# | **Uso pedag√≥gico recomendado**    | Para introducir conceptos b√°sicos de `RetrievalQA`  | Para ense√±ar buenas pr√°cticas actuales en trazabilidad y uso del LLM       |
# | **Estabilidad futura del c√≥digo** | Baja (usa m√≥dulos y m√©todos deprecados)             | Alta (adaptado a la versi√≥n estable y mantenida de LangChain)              |

# %% [markdown]
# üß† Diferencias clave entre ambos scripts
# 
# | Caracter√≠stica       | Script con `create_retrieval_chain` (moderno, script siguiente)            | Script con `RetrievalQA.from_chain_type` (cl√°sico, script anterior) |
# | -------------------- | -------------------------------------------------------- | -------------------------------------------------- |
# | API utilizada        | Moderna (modular, flexible)                              | Cl√°sica (m√°s sencilla, menos control)              |
# | Cadena usada         | `create_retrieval_chain` + `combine_docs_chain`          | `RetrievalQA.from_chain_type`                      |
# | Prompt               | Totalmente personalizado con `PromptTemplate`            | Interno, no configurable por defecto               |
# | Control de flujo     | Separaci√≥n clara de pasos (`retriever`, `prompt`, `LLM`) | Todo en una sola l√≠nea                             |
# | Adaptabilidad futura | Alta (recomendado en versiones recientes)                | Limitada (deprecated en versiones nuevas)          |
# 

# %% [markdown]
# ### 6.3. Con recuperaci√≥n moderna ‚Äì El sistema responde y muestra fuentes

# %%
from langchain.prompts import PromptTemplate
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.retrieval import create_retrieval_chain
from langchain_openai import ChatOpenAI

# 1. LLM moderno
llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0.0, api_key=api_key_environ)

# 2. Prompt simple para combinar documentos
prompt = PromptTemplate.from_template(
    "Usa los siguientes documentos para responder la pregunta.\n\n{context}\n\nPregunta: {input}"
)

# 3. Cadena para combinar documentos (tipo "stuff")
combine_docs_chain = create_stuff_documents_chain(
    llm=llm,
    prompt=prompt
)

# 4. Cadena de recuperaci√≥n completa con el retriever
qa_chain = create_retrieval_chain(
    retriever=retriever,
    combine_docs_chain=combine_docs_chain
)


# %%
response = qa_chain.invoke({"input": "aeropuerto del amazonas"})

print("üß† Respuesta:")
print(response["answer"])

print("\nüìö Documentos usados:")
for i, doc in enumerate(response["context"]):
    print(f"\nFuente {i+1}:", doc.metadata.get("source", "desconocido"))
    print(doc.page_content[:300])


# %% [markdown]
# üß† Diferencias clave entre ambos scripts
# 
# | Caracter√≠stica       | Script con `create_retrieval_chain` (moderno)            | Script con `RetrievalQA.from_chain_type` (cl√°sico) |
# | -------------------- | -------------------------------------------------------- | -------------------------------------------------- |
# | API utilizada        | Moderna (modular, flexible)                              | Cl√°sica (m√°s sencilla, menos control)              |
# | Cadena usada         | `create_retrieval_chain` + `combine_docs_chain`          | `RetrievalQA.from_chain_type`                      |
# | Prompt               | Totalmente personalizado con `PromptTemplate`            | Interno, no configurable por defecto               |
# | Control de flujo     | Separaci√≥n clara de pasos (`retriever`, `prompt`, `LLM`) | Todo en una sola l√≠nea                             |
# | Adaptabilidad futura | Alta (recomendado en versiones recientes)                | Limitada (deprecated en versiones nuevas)          |
# 

# %% [markdown]
# #### 6.3.1 Gradio para seccion 6.3.

# %%
import gradio as gr

# Funci√≥n para manejar la entrada y salida de QA Chain
def ask_question(question: str):
    # Invoca la cadena con la pregunta del usuario
    response = qa_chain.invoke({"input": question})
    # Extrae la respuesta
    answer = response.get("answer", "")
    # Construye la lista de fuentes y fragmentos
    context_snippets = []
    for i, doc in enumerate(response.get("context", [])):
        source = doc.metadata.get("source", "desconocido")
        snippet = doc.page_content[:300].replace("\n", " ")  # primeras 300 chars
        context_snippets.append(f"Fuente {i+1}: {source}\n{snippet}")
    context_text = "\n\n".join(context_snippets)
    return answer, context_text

# Definici√≥n de la interfaz de Gradio
iface = gr.Interface(
    fn=ask_question,
    inputs=gr.Textbox(lines=2, placeholder="Ingrese su pregunta aqu√≠‚Ä¶"),
    outputs=[
        gr.Textbox(lines=5, label="Respuesta"),
        gr.Textbox(lines=20,label="Documentos usados")
    ],
    title="Interfaz QA con LangChain",
    description="Ingrese una consulta y obtenga la respuesta junto con las fuentes utilizadas."
)

# Lanzar la aplicaci√≥n (en local o con share=True para exponerla)
iface.launch()
#iface.launch(share=True)


# %% [markdown]
# ### 6.4. Con memoria ‚Äì El sistema conserva el contexto

# %%
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

qa_chain = ConversationalRetrievalChain.from_llm(
    llm=ChatOpenAI(temperature=0),
    retriever=vectorstore.as_retriever(),
    memory=memory
)

# Simulaci√≥n de una conversaci√≥n con contexto
chat_history = []

# Primera pregunta
response_1 = qa_chain({"question": "¬øQu√© entidad est√° a cargo del contrato de la v√≠a Oca√±a-C√∫cuta?"})
print("Respuesta 1:", response_1['answer'])

# Segunda pregunta que se apoya en la anterior
response_2 = qa_chain({"question": "¬øCu√°l es el plazo de entrega del contrato?"})
print("Respuesta 2:", response_2['answer'])
# Aqu√≠, la segunda pregunta s√≠ aprovecha el contexto anterior, gracias a la memoria.

# %% [markdown]
# ## 7. LangChain para INVIAS
# 

# %% [markdown]
# ### 7.1. Descripci√≥n del c√≥digo

# %% [markdown]
# El archivo `langc_v01.ipynb` es un **notebook de Jupyter** dise√±ado para mostrar paso a paso c√≥mo construir un **sistema de consulta de informaci√≥n basado en documentos PDF usando LangChain y modelos de lenguaje de OpenAI**. 
# 
# #### Prop√≥sito del archivo `langc_v01.ipynb`
# 
# Implementar un **chatbot inteligente** que pueda:
# 
# 1. **Leer documentos PDF** institucionales.
# 2. **Convertirlos en embeddings sem√°nticos** usando OpenAI.
# 3. **Almacenarlos en una base vectorial persistente** (Chroma).
# 4. **Responder preguntas** formuladas en lenguaje natural usando LLMs.
# 
# #### Estructura del notebook
# 
# 1. **Carga y lectura de documentos PDF**
# 
#    * Usa loaders como `PyPDFLoader`.
#    * Fragmenta el contenido en chunks con metadatos.
#    * Prepara los datos para el embedding.
# 
# 2. **Generaci√≥n de embeddings y base vectorial**
# 
#    * Usa `OpenAIEmbeddings` (`text-embedding-ada-002`).
#    * Crea y guarda una base en Chroma (`chroma_db`).
#    * Permite que la b√∫squeda de contexto sea sem√°ntica, no solo por palabras clave.
# 
# 3. **Creaci√≥n del `retriever`**
# 
#    * Define cu√°ntos documentos relevantes recuperar (`k=3` o `k=5`).
#    * Es el n√∫cleo del sistema RAG (Retrieval-Augmented Generation).
# 
# 4. **Construcci√≥n de la cadena de QA (`RetrievalQA`)**
# 
#    * Conecta el `retriever` con un modelo de lenguaje (`ChatOpenAI`).
#    * Puede configurarse para devolver solo la respuesta, o tambi√©n las fuentes.
# 
# 5. **Ejecuci√≥n de consultas**
# 
#    * Env√≠a preguntas como `"¬øQu√© es CCPT?"` o `"¬øQu√© acciones tom√≥ el INV√çAS en El Tarrita?"`.
#    * El sistema responde bas√°ndose en los documentos cargados.
# 
# > Este codigo viene de [langc_v01.ipynb](../../../Platzi_codes/langc/v01/langc_v01.ipynb). Este c√≥digo desarrollo tiene [Code Explanation](../../../AI_Queries/code_explanation/).  

# %% [markdown]
# ### 7.2. Test del c√≥digo

# %% [markdown]
# - [QA_01.md](others/QA_01.md)
# - [QA_02.md](others/QA_02.md)
# - [QA_03.md](others/QA_03.md)

# %% [markdown]
# ### 7.3. Hallazgos üö©

# %% [markdown]
# 1. No cita las fuentes de donde extrae la informaci√≥n.
# 2. Solo para una pregunta. Si uso LangChain con un `ConversationalRetrievalChain`, se puede hacer varias preguntas en la misma sesi√≥n
# 3. Sin memoria ‚Äì Cada pregunta se responde de forma aislada


