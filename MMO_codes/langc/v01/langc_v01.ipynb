{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X11QtSmiMV9e"
   },
   "source": [
    "# LangChain para INVIAS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drFyYyIWMavB"
   },
   "source": [
    "## üîí 1. Instalaci√≥n de librer√≠as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîì  \n",
    "\n",
    "Solo ejecutar este script si no se han instalado los paquetes para desarrollar el c√≥digo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5901,
     "status": "ok",
     "timestamp": 1688165628049,
     "user": {
      "displayName": "Omar Espejel",
      "userId": "05134704829153205181"
     },
     "user_tz": 360
    },
    "id": "LofnVb23dSxe"
   },
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Lista de los paquetes a instalar:\n",
    "\n",
    "    upgrade:\n",
    "        pip\n",
    "        setuptools\n",
    "        wheel\n",
    "    Packages:\n",
    "        langchain \n",
    "        pypdf \n",
    "        openai \n",
    "        chromadb \n",
    "        tiktoken\n",
    "        langchain-community\n",
    "\"\"\"\n",
    "import subprocess\n",
    "\n",
    "comandos = [\n",
    "    [\"pip\", \"install\", \"--upgrade\", \"pip\", \"setuptools\", \"wheel\"],\n",
    "    [\"pip\", \"install\", \"langchain\", \"pypdf\", \"openai\", \"chromadb\", \"tiktoken\"],\n",
    "    [\"pip\", \"install\", \"-U\", \"langchain-community\"],\n",
    "    [\"python\", \"-m\", \"pip\", \"install\", \"--upgrade\", \"pip\"]\n",
    "]\n",
    "\n",
    "log_path = \"instalacion_log.txt\"\n",
    "\n",
    "with open(log_path, \"w\", encoding=\"utf-8\") as log_file:\n",
    "    for i, cmd in enumerate(comandos, start=1):\n",
    "        log_file.write(f\"\\nüîß Ejecutando comando {i}: {' '.join(cmd)}\\n\")\n",
    "        result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        \n",
    "        log_file.write(\"‚úÖ STDOUT:\\n\")\n",
    "        log_file.write(result.stdout + \"\\n\")\n",
    "        \n",
    "        if result.stderr:\n",
    "            log_file.write(\"‚ö†Ô∏è STDERR:\\n\")\n",
    "            log_file.write(result.stderr + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Resultado guardado en {log_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuraci√≥n de API Key de OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8057,
     "status": "ok",
     "timestamp": 1688165818015,
     "user": {
      "displayName": "Omar Espejel",
      "userId": "05134704829153205181"
     },
     "user_tz": 360
    },
    "id": "dHG9AVJkh3Dz",
    "outputId": "616af93a-257b-4cba-81d9-2d2797414792"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¬°Cliente de OpenAI inicializado correctamente!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "# Recuperar la clave API de la variable de entorno\n",
    "api_key_environ = os.environ.get(\"OPENAI_API_KEY\")\n",
    " \n",
    "# Verificar que la clave API est√© disponible\n",
    "if not api_key_environ:\n",
    "    raise ValueError(\"La variable de entorno OPENAI_API_KEY no est√° configurada o est√° vac√≠a.\")\n",
    " \n",
    "# Inicializar el cliente de OpenAI con la clave API\n",
    "client = OpenAI(api_key=api_key_environ)\n",
    " \n",
    "# Usar el cliente para tus tareas\n",
    "print(\"¬°Cliente de OpenAI inicializado correctamente!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4Z_Xi-GvMf8E"
   },
   "source": [
    "## üîí 3. Carga de documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîì  \n",
    "\n",
    "Solo ejecutar este script si no se ha hecho el proceso de embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 57463,
     "status": "ok",
     "timestamp": 1688165880499,
     "user": {
      "displayName": "Omar Espejel",
      "userId": "05134704829153205181"
     },
     "user_tz": 360
    },
    "id": "W_6B2k3Vcfxt",
    "outputId": "862c3d28-9656-41cc-ecde-31daec5f7a6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Cargando 2024S-VBOG-054699.pdf\n",
      "üìÑ Cargando 2024S-VBOG-056838.pdf\n",
      "üìÑ Cargando 2024S-VBOG-056839.pdf\n",
      "üìÑ Cargando 2024S-VBOG-056844.pdf\n",
      "üìÑ Cargando 2024S-VBOG-056845.pdf\n",
      "Esto es todo el contenido de `ml_papers:`\n",
      "\n",
      "üÜó Todos los documentos estan cargados en ml_papers.\n",
      "‚ûñ Total de fragmentos: 31\n",
      "‚ûñ Los fragmentos son cada una de las hojas de cada uno de los 5 archivos en la carpeta ../../../assets/DG_docs/PDFs_test/\n",
      "‚ûñ Este script se ejecuta desde c:\\Users\\devel\\UNAD\\INVIAS\\INVIAS_NLP\\MMO_codes\\langc\\v01\n",
      "‚ûñ Este es el contenido de la √∫ltima hoja cargada page_content='d. Las tasas de peajes ser√°n diferenciales, es decir, se fijar√°n en proporci√≥n a las\n",
      "distancias recorridas, las caracter√≠sticas vehiculares y sus respectivos costos de\n",
      "operaci√≥n. \n",
      "e. Para la determinaci√≥n del valor del peaje y de las tasas de valorizaci√≥n, en las\n",
      "v√≠as nacionales, se tendr√° en cuenta un criterio de equidad fiscal‚Äù.\n",
      "4. S√≠rvase dar a conocer cu√°les son los servicios que por normativa est√°n\n",
      "obligados a brindar las concesiones  a la ciudadan√≠a (rescate, asistencia,\n",
      "etc.) de forma permanente. \n",
      "Tal y como se inform√≥ previamente, la red vial concesionada se encuentra a cargo de la\n",
      "Agencia Nacional  de  Infraestructura, Entidad  que  incluyen  dentro de las obligaciones\n",
      "contractuales de la concesi√≥n la prestaci√≥n de este tipo de servicios a la ciudadan√≠a, tales\n",
      "como, ambulancias, carro talleres y gr√∫as; en raz√≥n a esto, mediante oficio 2024S-VBOG-\n",
      "054509 se le dio traslado por competencia.\n",
      "De otra parte, la red vial a cargo del INV√çAS no es concesionada; adicionalmente, no\n",
      "existe norma alguna que especifique la prestaci√≥n de servicios asociados al usuario de la\n",
      "v√≠a, en raz√≥n a lo anterior, el INV√çAS no desarrolla los proyectos con alcance de atenci√≥n\n",
      "con ambulancias, carro talleres y gr√∫as, que est√©n asociados al pago de la tasa de peaje,\n",
      "toda vez que la misma tiene su origen en el uso de la infraestructura vial.\n",
      "5. S√≠rvase dar a conocer si por parte de su entidad se tiene planeado establecer\n",
      "un peaje entre Bogot√° y Soacha para la financiaci√≥n del proyecto Avenida\n",
      "Longitudinal de Occidente tramo SUR ‚Äì ALO SUR.\n",
      "Por tratarse la ALO SUR de un proyecto de concesi√≥n a cargo de la Agencia Nacional de\n",
      "Infraestructura ‚Äì ANI, el Instituto Nacional de V√≠as ‚Äì INV√çAS no tiene relaci√≥n con la\n",
      "instalaci√≥n de peajes en el corredor Bogot√° ‚Äì Soacha, por lo anterior mediante oficio\n",
      "2024S-VBOG-054509 se dio traslado por competencia a la ANI.\n",
      "Atentamente, \n",
      "JUAN CARLOS MONTENEGRO ARJONA\n",
      "DIRECTOR GENERAL\n",
      "Proyectado Por: EDNA SAMANTHA RODRIGUEZ PINZON\n",
      "Revisado Por:  ANNY YIRLESA ARIAS SALAZAR, JOSE MANUEL GOMEZ DUQUE, LAURA GERALDINE SANDOVAL\n",
      "BARRERA, MAURICIO HERNAN CESPEDES SOLANO, RUBY AMPARO  MALAVER MONTA√ëA\n",
      "Aprobado Por: LAURA GERALDINE SANDOVAL BARRERA, MAURICIO HERNAN CESPEDES SOLANO, RUBY AMPARO\n",
      " MALAVER MONTA√ëA\n",
      "Copia Interna a: MARISOL ANDRADE MARTINEZ (1000000)\n",
      "Copia Externa a: JUAN DAVID GARZON BAHAMON (MINISTERIO DE TRANSPORTE), GRUPO ENLACE CONGRESO\n",
      "MINTRANSPORTE  (MINISTERIO  DE  TRANSPORTE),  ASUNTOS  LEGISLATIVOS  ANI  (AGENCIA  NACIONAL  DE\n",
      "INFRAESTRUCTURA ANI)\n",
      "_____________________________________________________________________________________________________\n",
      "INSTITUTO NACIONAL DE V√çAS P√°gina 4 | 4\n",
      "Direcci√≥n: Calle 25G # 73B - 90, Bogot√° D.C., Colombia\n",
      "PBX: (+57) 601 377 0600 L√≠nea gratuita: 018000117844\n",
      "Correo institucional: atencionciudadano@invias.gov.co\n",
      "http://www.invias.gov.co' metadata={'producer': 'LibreOffice 5.2', 'creator': 'Writer', 'creationdate': '2024-08-17T01:39:05+00:00', 'author': 'Neiffeth Julieth Vargas Vargas', 'source': '../../../assets/DG_docs/PDFs_test/2024S-VBOG-056845.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "import os\n",
    "\n",
    "relative_pdf_path = \"../../../assets/DG_docs/PDFs_test/\"\n",
    "\n",
    "ml_papers = []\n",
    "\n",
    "for i, file_name in enumerate(os.listdir(relative_pdf_path)):\n",
    "    if file_name.lower().endswith(\".pdf\"):\n",
    "        full_pdf_path = os.path.join(relative_pdf_path,file_name)\n",
    "        print(f\"üìÑ Cargando {file_name}\")\n",
    "\n",
    "        loader = PyPDFLoader(full_pdf_path)\n",
    "        data = loader.load() # AI_Queries\\code_explanation\\ai_query-langc_v01-PyPDFLoader(filename).loader.load()_usage.md\n",
    "        ml_papers.extend(data) # AI_Queries\\code_explanation\\ai_query-langc_v01-.extend_usage.md\n",
    "        # print (ml_papers) # AI_Queries/code_explanation/ai_query-langc_v01-list_start_end_usage.md\n",
    "# Utiliza la lista ml_papers para acceder a los elementos de todos los documentos descargados\n",
    "print('Esto es todo el contenido de `ml_papers:`')\n",
    "print(f\"\"\"\n",
    "üÜó Todos los documentos estan cargados en ml_papers.\n",
    "‚ûñ Total de fragmentos: {len(ml_papers)}\n",
    "‚ûñ Los fragmentos son cada una de las hojas de cada uno de los {len(os.listdir(relative_pdf_path))} archivos en la carpeta {relative_pdf_path}\n",
    "‚ûñ Este script se ejecuta desde {os.getcwd()}\n",
    "‚ûñ Este es el contenido de la √∫ltima hoja cargada {ml_papers[-1]}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJYjDA_GMi0Z"
   },
   "source": [
    "## üîí 4. Split de documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîì\n",
    "\n",
    "Solo ejecutar este script para desarrollar el proceso de embedding; este script depende del script anterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1688165880500,
     "user": {
      "displayName": "Omar Espejel",
      "userId": "05134704829153205181"
     },
     "user_tz": 360
    },
    "id": "4caTdNe-hk7w"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, \n",
    "    # AI_Queries\\code_explanation\\ai_query-langc_v01-chunk_usage.md \n",
    "    # AI_Queries\\code_explanation\\ai_query-langc_v01-max_tokens_Chatgptmodels.md \n",
    "    # AI_Queries\\code_explanation\\ai_query-langc_v01-meaning_inputpromptandanswer.md \n",
    "    # AI_Queries\\code_explanation\\ai_query-langc_v01-retrieval_meaning.md\n",
    "    \n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    "    )\n",
    "\n",
    "documents = text_splitter.split_documents(ml_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1688165880500,
     "user": {
      "displayName": "Omar Espejel",
      "userId": "05134704829153205181"
     },
     "user_tz": 360
    },
    "id": "koi4gwzthsGh",
    "outputId": "6ed35c64-9026-4222-b86a-379809bfb6dc"
   },
   "outputs": [],
   "source": [
    "len(documents), documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîí 5. Embeddings e ingesta a base de datos vectorial \n",
    "\n",
    "‚ö†Ô∏è advertencia de uso de esta secci√≥n ‚ö†Ô∏è"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîì 5.1.\n",
    "\n",
    "Solo ejecutar este script para desarrolla el proceso de embedding; este script depende del script anterior\n",
    "\n",
    "**Aqu√≠ se consume recurso de la API de OpenAI**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6756,
     "status": "ok",
     "timestamp": 1688165966034,
     "user": {
      "displayName": "Omar Espejel",
      "userId": "05134704829153205181"
     },
     "user_tz": 360
    },
    "id": "iZ-ZFWgRh9aV"
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# AI_Queries\\code_explanation\\ai_query-langc_v01-Embeddings_and_Vector_Store_Ingestion.md\n",
    "\n",
    "# 1. Crear embeddings con el modelo oficial de OpenAI\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\") # ‚ö†Ô∏è cambiar a \"text-embedding-3-small\"\n",
    "\n",
    "# 2. Definir carpeta para almacenar la base de datos vectorial\n",
    "persist_directory = \"chroma_db\" #\n",
    "\n",
    "# 3. Crear la base desde documentos y embeddings\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=persist_directory\n",
    ")\n",
    "\n",
    "# 4. Guardar la base en disco\n",
    "vectorstore.persist()\n",
    "print(\"‚úÖ Base de datos Chroma guardada en:\", persist_directory)\n",
    "\n",
    "\n",
    "# 5. Cargar la base vectorial guardada en disco\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    "\n",
    "# 6. Usar como retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 3}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.\n",
    "\n",
    "Fue necesario crear copia de las lineas \"*# 1. Crear embeddings con el modelo oficial de OpenAI*\", \"*# 5. Cargar la base vectorial guardada en disco*\" y \"*# 6. Usar como retriever*\" para **solamente hacer uso de la Based de Datos de embeddings `chroma_db` ya creada y evitar recalcularla**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devel\\AppData\\Local\\Temp\\ipykernel_13320\\2833968785.py:5: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAIEmbeddings``.\n",
      "  embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")  # ‚ö†Ô∏è cambiar a \"text-embedding-3-small\"\n",
      "C:\\Users\\devel\\AppData\\Local\\Temp\\ipykernel_13320\\2833968785.py:8: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vectorstore = Chroma(\n"
     ]
    }
   ],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "# 1. Modelo de embeddings (debe ser el mismo usado al crear la base)\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")  # ‚ö†Ô∏è cambiar a \"text-embedding-3-small\"\n",
    "\n",
    "# 5. Cargar la base vectorial guardada en disco\n",
    "vectorstore = Chroma(\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    "\n",
    "# 6. Usar como retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBbAhkFKMrDC"
   },
   "source": [
    "## 6. Modelos de chat y cadenas para consulta de informaci√≥n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö© 6.1. Recuperaci√≥n sin trazabilidad ‚Äì Primer acercamiento con LangChain; ~~No cita las fuentes de donde extrae la informaci√≥n~~."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devel\\AppData\\Local\\Temp\\ipykernel_184\\171208245.py:6: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  chat = ChatOpenAI(\n"
     ]
    }
   ],
   "source": [
    "#AI_Queries/code_explanation/ai_query-langc_v01-Chat_Models_and_Retrieval_Chains_for_Information_Querying.md\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    openai_api_key=api_key_environ,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "# AI_Queries/code_explanation/ai_query-langc_v01-RetrievalQA.from_chain_type_usage.md\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CCPT se refiere al Programa Caminos Comunitarios para la Paz Total. Es un programa orientado a la identificaci√≥n de necesidades en la red vial regional del pa√≠s, donde cualquier Organismo de Acci√≥n Comunal, comunidad √©tnica o entidad sin √°nimo de lucro puede presentar sus necesidades para la planeaci√≥n y estructuraci√≥n de los tramos a intervenir, con la participaci√≥n activa de la comunidad.'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# AI_Queries/code_explanation/ai_query-langc_v01-qa_chain.run()_usage.md\n",
    "\n",
    "query = \"qu√© es CCPT?\"\n",
    "qa_chain.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Comparaci√≥n entre Script 1 y Script 2\n",
    "\n",
    "| Aspecto                           | **Script anterior**                                 | **Script siguiente**                                                       |\n",
    "| --------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| **Importaci√≥n del modelo**        | `from langchain.chat_models` (obsoleto)             | `from langchain_openai` (recomendado)                                      |\n",
    "| **Versi√≥n del API**               | Antiguo LangChain monol√≠tico                        | Nuevo ecosistema modular `langchain-openai`                                |\n",
    "| **M√©todo de ejecuci√≥n**           | `qa_chain.run(query)` (‚ö†Ô∏è obsoleto)                 | `qa_chain.invoke({\"query\": query})` (‚úÖ recomendado)                        |\n",
    "| **Fuentes del resultado**         | ‚ùå No devuelve los documentos fuente                 | ‚úÖ Incluye los documentos fuente con `return_source_documents=True`         |\n",
    "| **Retorno estructurado**          | S√≥lo devuelve texto plano                           | Devuelve un diccionario con respuesta y `source_documents`                 |\n",
    "| **Transparencia institucional**   | Baja (no se puede verificar origen de la respuesta) | Alta (permite validar en qu√© parte de los documentos se basa la respuesta) |\n",
    "| **Uso pedag√≥gico recomendado**    | Para introducir conceptos b√°sicos de `RetrievalQA`  | Para ense√±ar buenas pr√°cticas actuales en trazabilidad y uso del LLM       |\n",
    "| **Estabilidad futura del c√≥digo** | Baja (usa m√≥dulos y m√©todos deprecados)             | Alta (adaptado a la versi√≥n estable y mantenida de LangChain)              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (üö©Revisar por qu√© no est√° gerendo resultadosüö©) 6.2. Con recuperaci√≥n b√°sica ‚Äì El sistema responde y cita fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 13,
     "status": "ok",
     "timestamp": 1688165966035,
     "user": {
      "displayName": "Omar Espejel",
      "userId": "05134704829153205181"
     },
     "user_tz": 360
    },
    "id": "fxjg2e6RiTqO"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Inicializa el modelo de lenguaje\n",
    "chat = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.0,\n",
    "    openai_api_key=api_key_environ\n",
    ")\n",
    "\n",
    "# Crear la cadena de preguntas y respuestas con fuentes\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=chat,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True  # ‚úÖ Para obtener las fuentes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "executionInfo": {
     "elapsed": 6199,
     "status": "ok",
     "timestamp": 1688165977030,
     "user": {
      "displayName": "Omar Espejel",
      "userId": "05134704829153205181"
     },
     "user_tz": 360
    },
    "id": "qgDyf-lOimCT",
    "outputId": "18f9ce42-2c78-4540-e6d3-ec5f01223cbb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Respuesta:\n",
      "No tengo informaci√≥n espec√≠fica sobre lo que est√° ocurriendo en Oca√±a en este momento.\n",
      "\n",
      "üìö Fuentes:\n",
      "\n",
      "Fuente 1:\n",
      "Archivo: ../../../assets/DG_docs/PDFs_test/2024S-VBOG-056839.pdf\n",
      "Contenido: Al contestar cite este radicado: 2024S-VBOG-056839\n",
      "Fecha: 2024-08-16 20:22:31\n",
      "DOCTOR:\n",
      "HR DI√ìGENES QUINTERO AMAYA\n",
      "REPRESENTANTE A LA C√ÅMARA\n",
      "CONGRESO DE LA REP√öBLICA\n",
      "CRA 7 # 8-68 Of. 232B Edificio Nuevo del Congreso\n",
      "diogenes.quintero@camara.gov.co\n",
      "BOGOT√Å, D.C.\n",
      "Asunto: Respuesta a los radicados 2024E-VUVRAZ-069741, 2024E-VUVRAZ-070800,\n",
      "2024E-VUVRAZ-070926 (Traslados con radicado MT No. 2024108089680)\n",
      "\n",
      "Fuente 2:\n",
      "Archivo: ../../../assets/DG_docs/PDFs_test/2024S-VBOG-056839.pdf\n",
      "Contenido: Al contestar cite este radicado: 2024S-VBOG-056839\n",
      "Fecha: 2024-08-16 20:22:31\n",
      "DOCTOR:\n",
      "HR DI√ìGENES QUINTERO AMAYA\n",
      "REPRESENTANTE A LA C√ÅMARA\n",
      "CONGRESO DE LA REP√öBLICA\n",
      "CRA 7 # 8-68 Of. 232B Edificio Nuevo del Congreso\n",
      "diogenes.quintero@camara.gov.co\n",
      "BOGOT√Å, D.C.\n",
      "Asunto: Respuesta a los radicados 2024E-VUVRAZ-069741, 2024E-VUVRAZ-070800,\n",
      "2024E-VUVRAZ-070926 (Traslados con radicado MT No. 2024108089680)\n",
      "\n",
      "Fuente 3:\n",
      "Archivo: ../../../assets/DG_docs/PDFs_test/2024S-VBOG-056839.pdf\n",
      "Contenido: ÔÇ∑ Proporcione copias de las evaluaciones de riesgo y estudios t√©cnicos que se\n",
      "han realizado sobre la v√≠a Oca√±a - C√∫cuta, especialmente en el sector de El\n",
      "Tarrita.\n",
      "_____________________________________________________________________________________________________\n",
      "INSTITUTO NACIONAL DE V√çAS P√°gina 6 | 8\n",
      "Direcci√≥n: Calle 25G # 73B - 90, Bogot√° D.C., Colombia\n",
      "PBX: (+57) 601 377 0600 L√≠nea gratuita: \n",
      "\n",
      "Fuente 4:\n",
      "Archivo: ../../../assets/DG_docs/PDFs_test/2024S-VBOG-056839.pdf\n",
      "Contenido: ÔÇ∑ Proporcione copias de las evaluaciones de riesgo y estudios t√©cnicos que se\n",
      "han realizado sobre la v√≠a Oca√±a - C√∫cuta, especialmente en el sector de El\n",
      "Tarrita.\n",
      "_____________________________________________________________________________________________________\n",
      "INSTITUTO NACIONAL DE V√çAS P√°gina 6 | 8\n",
      "Direcci√≥n: Calle 25G # 73B - 90, Bogot√° D.C., Colombia\n",
      "PBX: (+57) 601 377 0600 L√≠nea gratuita: \n",
      "\n",
      "Fuente 5:\n",
      "Archivo: ../../../assets/DG_docs/PDFs_test/2024S-VBOG-056839.pdf\n",
      "Contenido: del Pozo, c√≥digo 7008, calzada sencilla, carril izquierdo. De igual forma en el\n",
      "PR 51+0500 adem√°s de la construcci√≥n del sistema de protecci√≥n articulado\n",
      "con hidro malla tipo geoestera para control de socavaci√≥n del muro que se\n",
      "construy√≥ con bolsacretos a nivel de mitigaci√≥n.\n",
      "o Construcci√≥n de obras de drenaje, terrapl√©n y estructura de pavimento con\n",
      "mezcla  asf√°ltica  en  caliente sobre  la  calz\n"
     ]
    }
   ],
   "source": [
    "query = \" que pasa en Oca√±a?\"\n",
    "result = qa_chain.invoke({\"query\": query})\n",
    "\n",
    "# Imprimir respuesta\n",
    "print(\"üß† Respuesta:\")\n",
    "print(result['result'])\n",
    "\n",
    "# Imprimir fuentes\n",
    "print(\"\\nüìö Fuentes:\")\n",
    "for i, doc in enumerate(result['source_documents']):\n",
    "    print(f\"\\nFuente {i+1}:\")\n",
    "    print(\"Archivo:\", doc.metadata.get(\"source\", \"desconocido\"))\n",
    "    print(\"Contenido:\", doc.page_content[:400])  # Muestra primeros 400 caracteres\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Comparaci√≥n entre Script 1 y Script 2\n",
    "\n",
    "| Aspecto                           | **Script anterior**                                 | **Script actual**                                                       |\n",
    "| --------------------------------- | --------------------------------------------------- | -------------------------------------------------------------------------- |\n",
    "| **Importaci√≥n del modelo**        | `from langchain.chat_models` (obsoleto)             | `from langchain_openai` (recomendado)                                      |\n",
    "| **Versi√≥n del API**               | Antiguo LangChain monol√≠tico                        | Nuevo ecosistema modular `langchain-openai`                                |\n",
    "| **M√©todo de ejecuci√≥n**           | `qa_chain.run(query)` (‚ö†Ô∏è obsoleto)                 | `qa_chain.invoke({\"query\": query})` (‚úÖ recomendado)                        |\n",
    "| **Fuentes del resultado**         | ‚ùå No devuelve los documentos fuente                 | ‚úÖ Incluye los documentos fuente con `return_source_documents=True`         |\n",
    "| **Retorno estructurado**          | S√≥lo devuelve texto plano                           | Devuelve un diccionario con respuesta y `source_documents`                 |\n",
    "| **Transparencia institucional**   | Baja (no se puede verificar origen de la respuesta) | Alta (permite validar en qu√© parte de los documentos se basa la respuesta) |\n",
    "| **Uso pedag√≥gico recomendado**    | Para introducir conceptos b√°sicos de `RetrievalQA`  | Para ense√±ar buenas pr√°cticas actuales en trazabilidad y uso del LLM       |\n",
    "| **Estabilidad futura del c√≥digo** | Baja (usa m√≥dulos y m√©todos deprecados)             | Alta (adaptado a la versi√≥n estable y mantenida de LangChain)              |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Diferencias clave entre ambos scripts\n",
    "\n",
    "| Caracter√≠stica       | Script con `create_retrieval_chain` (moderno, script siguiente)            | Script con `RetrievalQA.from_chain_type` (cl√°sico, script anterior) |\n",
    "| -------------------- | -------------------------------------------------------- | -------------------------------------------------- |\n",
    "| API utilizada        | Moderna (modular, flexible)                              | Cl√°sica (m√°s sencilla, menos control)              |\n",
    "| Cadena usada         | `create_retrieval_chain` + `combine_docs_chain`          | `RetrievalQA.from_chain_type`                      |\n",
    "| Prompt               | Totalmente personalizado con `PromptTemplate`            | Interno, no configurable por defecto               |\n",
    "| Control de flujo     | Separaci√≥n clara de pasos (`retriever`, `prompt`, `LLM`) | Todo en una sola l√≠nea                             |\n",
    "| Adaptabilidad futura | Alta (recomendado en versiones recientes)                | Limitada (deprecated en versiones nuevas)          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3. Con recuperaci√≥n moderna ‚Äì El sistema responde y muestra fuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 1. LLM moderno\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0.0, api_key=api_key_environ)\n",
    "\n",
    "# 2. Prompt simple para combinar documentos\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Usa los siguientes documentos para responder la pregunta.\\n\\n{context}\\n\\nPregunta: {input}\"\n",
    ")\n",
    "\n",
    "# 3. Cadena para combinar documentos (tipo \"stuff\")\n",
    "combine_docs_chain = create_stuff_documents_chain(\n",
    "    llm=llm,\n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "# 4. Cadena de recuperaci√≥n completa con el retriever\n",
    "qa_chain = create_retrieval_chain(\n",
    "    retriever=retriever,\n",
    "    combine_docs_chain=combine_docs_chain\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Respuesta:\n",
      "1. Radicado 2024S-VBOG-054699\n",
      "2. Documento de respuesta al radicado 2024E-VUVRAZ-067052\n",
      "3. Documento de convocatoria p√∫blica del Programa CCPT realizado por el INV√çAS\n",
      "\n",
      "üìö Documentos usados:\n",
      "\n",
      "Fuente 1: ../../../assets/DG_docs/PDFs_test/2024S-VBOG-054699.pdf\n",
      "Al contestar cite este radicado: 2024S-VBOG-054699\n",
      "Fecha: 2024-08-12 09:38:46\n",
      "DOCTOR:\n",
      "HR CARLOS ALBERTO CARRE√ëO MAR√çN\n",
      "REPRESENTANTE \n",
      "CONGRESO DE LA REP√öBLICA\n",
      "Carrera 7 No. 8-68 - Edificio Nuevo del Congreso\n",
      "carlos.carreno@camara.gov.co\n",
      "BOGOT√Å, D.C.\n",
      "Asunto: Respuesta al radicado 2024E-VUVRAZ-067052.\n",
      "\n",
      "\n",
      "Fuente 2: ../../../assets/DG_docs/PDFs_test/2024S-VBOG-054699.pdf\n",
      "Al contestar cite este radicado: 2024S-VBOG-054699\n",
      "Fecha: 2024-08-12 09:38:46\n",
      "DOCTOR:\n",
      "HR CARLOS ALBERTO CARRE√ëO MAR√çN\n",
      "REPRESENTANTE \n",
      "CONGRESO DE LA REP√öBLICA\n",
      "Carrera 7 No. 8-68 - Edificio Nuevo del Congreso\n",
      "carlos.carreno@camara.gov.co\n",
      "BOGOT√Å, D.C.\n",
      "Asunto: Respuesta al radicado 2024E-VUVRAZ-067052.\n",
      "\n",
      "\n",
      "Fuente 3: ../../../assets/DG_docs/PDFs_test/2024S-VBOG-054699.pdf\n",
      "comunican el municipio con las veredas mencionadas anteriormente (‚Ä¶)‚Äù. \n",
      "La convocatoria p√∫blica en el  marco del Programa CCPT adelantada  por el INV√çAS,\n",
      "estuvo orientada a la identificaci√≥n de las necesidades en la red vial regional (terciaria y\n",
      "secundaria)  del  pa√≠s,  para  lo  cual  cualquier  O\n",
      "\n",
      "Fuente 4: ../../../assets/DG_docs/PDFs_test/2024S-VBOG-054699.pdf\n",
      "comunican el municipio con las veredas mencionadas anteriormente (‚Ä¶)‚Äù. \n",
      "La convocatoria p√∫blica en el  marco del Programa CCPT adelantada  por el INV√çAS,\n",
      "estuvo orientada a la identificaci√≥n de las necesidades en la red vial regional (terciaria y\n",
      "secundaria)  del  pa√≠s,  para  lo  cual  cualquier  O\n",
      "\n",
      "Fuente 5: ../../../assets/DG_docs/PDFs_test/2024S-VBOG-056838.pdf\n",
      "encontraban  incluidos  dentro  de  los  alcances  primarios  del  proyecto  y\n",
      "est√°n ubicados en las siguientes abscisas: puente vehicular PR26+600\n",
      "_____________________________________________________________________________________________________\n",
      "INSTITUTO NACIONAL DE V√çAS P√°gina 6 | 15\n",
      "Direcci√≥n\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain.invoke({\"input\": \" deme un listado de los documentos donde se menciona CCPT?\"})\n",
    "\n",
    "print(\"üß† Respuesta:\")\n",
    "print(response[\"answer\"])\n",
    "\n",
    "print(\"\\nüìö Documentos usados:\")\n",
    "for i, doc in enumerate(response[\"context\"]):\n",
    "    print(f\"\\nFuente {i+1}:\", doc.metadata.get(\"source\", \"desconocido\"))\n",
    "    print(doc.page_content[:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üß† Diferencias clave entre ambos scripts\n",
    "\n",
    "| Caracter√≠stica       | Script con `create_retrieval_chain` (moderno)            | Script con `RetrievalQA.from_chain_type` (cl√°sico) |\n",
    "| -------------------- | -------------------------------------------------------- | -------------------------------------------------- |\n",
    "| API utilizada        | Moderna (modular, flexible)                              | Cl√°sica (m√°s sencilla, menos control)              |\n",
    "| Cadena usada         | `create_retrieval_chain` + `combine_docs_chain`          | `RetrievalQA.from_chain_type`                      |\n",
    "| Prompt               | Totalmente personalizado con `PromptTemplate`            | Interno, no configurable por defecto               |\n",
    "| Control de flujo     | Separaci√≥n clara de pasos (`retriever`, `prompt`, `LLM`) | Todo en una sola l√≠nea                             |\n",
    "| Adaptabilidad futura | Alta (recomendado en versiones recientes)                | Limitada (deprecated en versiones nuevas)          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4. Con memoria ‚Äì El sistema conserva el contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\devel\\AppData\\Local\\Temp\\ipykernel_4176\\1190348934.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta 1: El contrato de la v√≠a Oca√±a-C√∫cuta est√° a cargo del Instituto Nacional de V√≠as (INV√çAS).\n",
      "Respuesta 2: El plazo de entrega del contrato de la v√≠a Oca√±a-C√∫cuta es desde el 23 de junio de 2023 hasta el 30 de septiembre de 2024.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=ChatOpenAI(temperature=0),\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# Simulaci√≥n de una conversaci√≥n con contexto\n",
    "chat_history = []\n",
    "\n",
    "# Primera pregunta\n",
    "response_1 = qa_chain({\"question\": \"¬øQu√© entidad est√° a cargo del contrato de la v√≠a Oca√±a-C√∫cuta?\"})\n",
    "print(\"Respuesta 1:\", response_1['answer'])\n",
    "\n",
    "# Segunda pregunta que se apoya en la anterior\n",
    "response_2 = qa_chain({\"question\": \"¬øCu√°l es el plazo de entrega del contrato?\"})\n",
    "print(\"Respuesta 2:\", response_2['answer'])\n",
    "# Aqu√≠, la segunda pregunta s√≠ aprovecha el contexto anterior, gracias a la memoria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Gradio para seccion 6.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Funci√≥n para manejar la entrada y salida de QA Chain\n",
    "def ask_question(question: str):\n",
    "    # Invoca la cadena con la pregunta del usuario\n",
    "    response = qa_chain.invoke({\"input\": question})\n",
    "    # Extrae la respuesta\n",
    "    answer = response.get(\"answer\", \"\")\n",
    "    # Construye la lista de fuentes y fragmentos\n",
    "    context_snippets = []\n",
    "    for i, doc in enumerate(response.get(\"context\", [])):\n",
    "        source = doc.metadata.get(\"source\", \"desconocido\")\n",
    "        snippet = doc.page_content[:300].replace(\"\\n\", \" \")  # primeras 300 chars\n",
    "        context_snippets.append(f\"Fuente {i+1}: {source}\\n{snippet}\")\n",
    "    context_text = \"\\n\\n\".join(context_snippets)\n",
    "    return answer, context_text\n",
    "\n",
    "# Definici√≥n de la interfaz de Gradio\n",
    "iface = gr.Interface(\n",
    "    fn=ask_question,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"Ingrese su pregunta aqu√≠‚Ä¶\"),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Respuesta\"),\n",
    "        gr.Textbox(label=\"Documentos usados\")\n",
    "    ],\n",
    "    title=\"Interfaz QA con LangChain\",\n",
    "    description=\"Ingrese una consulta y obtenga la respuesta junto con las fuentes utilizadas.\"\n",
    ")\n",
    "\n",
    "# Lanzar la aplicaci√≥n (en local o con share=True para exponerla)\n",
    "iface.launch()\n",
    "#iface.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. LangChain para INVIAS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1. Descripci√≥n del c√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El archivo `langc_v01.ipynb` es un **notebook de Jupyter** dise√±ado para mostrar paso a paso c√≥mo construir un **sistema de consulta de informaci√≥n basado en documentos PDF usando LangChain y modelos de lenguaje de OpenAI**. \n",
    "\n",
    "#### Prop√≥sito del archivo `langc_v01.ipynb`\n",
    "\n",
    "Implementar un **chatbot inteligente** que pueda:\n",
    "\n",
    "1. **Leer documentos PDF** institucionales.\n",
    "2. **Convertirlos en embeddings sem√°nticos** usando OpenAI.\n",
    "3. **Almacenarlos en una base vectorial persistente** (Chroma).\n",
    "4. **Responder preguntas** formuladas en lenguaje natural usando LLMs.\n",
    "\n",
    "#### Estructura del notebook\n",
    "\n",
    "1. **Carga y lectura de documentos PDF**\n",
    "\n",
    "   * Usa loaders como `PyPDFLoader`.\n",
    "   * Fragmenta el contenido en chunks con metadatos.\n",
    "   * Prepara los datos para el embedding.\n",
    "\n",
    "2. **Generaci√≥n de embeddings y base vectorial**\n",
    "\n",
    "   * Usa `OpenAIEmbeddings` (`text-embedding-ada-002`).\n",
    "   * Crea y guarda una base en Chroma (`chroma_db`).\n",
    "   * Permite que la b√∫squeda de contexto sea sem√°ntica, no solo por palabras clave.\n",
    "\n",
    "3. **Creaci√≥n del `retriever`**\n",
    "\n",
    "   * Define cu√°ntos documentos relevantes recuperar (`k=3` o `k=5`).\n",
    "   * Es el n√∫cleo del sistema RAG (Retrieval-Augmented Generation).\n",
    "\n",
    "4. **Construcci√≥n de la cadena de QA (`RetrievalQA`)**\n",
    "\n",
    "   * Conecta el `retriever` con un modelo de lenguaje (`ChatOpenAI`).\n",
    "   * Puede configurarse para devolver solo la respuesta, o tambi√©n las fuentes.\n",
    "\n",
    "5. **Ejecuci√≥n de consultas**\n",
    "\n",
    "   * Env√≠a preguntas como `\"¬øQu√© es CCPT?\"` o `\"¬øQu√© acciones tom√≥ el INV√çAS en El Tarrita?\"`.\n",
    "   * El sistema responde bas√°ndose en los documentos cargados.\n",
    "\n",
    "> Este codigo viene de [langc_v01.ipynb](../../../Platzi_codes/langc/v01/langc_v01.ipynb). Este c√≥digo desarrollo tiene [Code Explanation](../../../AI_Queries/code_explanation/).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2. Test del c√≥digo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [QA_01.md](others/QA_01.md)\n",
    "- [QA_02.md](others/QA_02.md)\n",
    "- [QA_03.md](others/QA_03.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3. Hallazgos üö©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. No cita las fuentes de donde extrae la informaci√≥n.\n",
    "2. Solo para una pregunta. Si uso LangChain con un `ConversationalRetrievalChain`, se puede hacer varias preguntas en la misma sesi√≥n\n",
    "3. Sin memoria ‚Äì Cada pregunta se responde de forma aislada"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1fm4uYP20LHFFkWTsxHRGaaL-p1bVLvG6",
     "timestamp": 1688058575617
    }
   ]
  },
  "kernelspec": {
   "display_name": "UNAD-dBjyLoWd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
